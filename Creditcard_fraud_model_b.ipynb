{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "901196e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd  \n",
    "import numpy as np  \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f65005aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in file\n",
    "nRowsRead = 284807\n",
    "df1 = pd.read_csv('C:\\\\Users\\\\ajens\\\\OneDrive\\\\Documents\\\\Personal\\\\A.Jensen MDS\\\\creditcard.csv\\\\creditcard.csv', delimiter=',', nrows= nRowsRead)\n",
    "df1.dataframeName = 'Fraud Detection'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "09e62099",
   "metadata": {},
   "outputs": [],
   "source": [
    "Class = pd.DataFrame(df1['Class'],columns=['Class'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e7393bef",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.drop(['Time', 'Class'], axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b7152bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.feature_selection import SelectPercentile\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f25f8a38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9985838512224524"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe = make_pipeline(\n",
    "    MinMaxScaler(),\n",
    "    GradientBoostingClassifier(random_state=42,\n",
    "                             n_estimators=100,\n",
    "                             verbose=False)\n",
    ")\n",
    "X_train, X_test, y_train, y_test = train_test_split(df1, Class, test_size=0.3, random_state=42)\n",
    "pipe.fit(X_train, y_train).score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "a5839301",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline(steps=[('minmaxscaler', MinMaxScaler()),\n",
      "                ('gradientboostingclassifier',\n",
      "                 GradientBoostingClassifier(random_state=42, verbose=False))]) \n",
      "\n",
      "Accuracy:  99.85838512224524\n"
     ]
    }
   ],
   "source": [
    "y_pred = pipe.predict(X_test)\n",
    "print(pipe,\"\\n\\nAccuracy: \",accuracy_score(y_test,ypred)*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86c1d091",
   "metadata": {},
   "source": [
    "The main problem of imbalanced data sets lies on the fact that they are often associated with a user preference bias towards the performance on cases that are poorly represented in the available data sample. We will see a better representation of our metrics after we balance our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "6760871c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[85305     2]\n",
      " [  119    17]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     85307\n",
      "           1       0.89      0.12      0.22       136\n",
      "\n",
      "    accuracy                           1.00     85443\n",
      "   macro avg       0.95      0.56      0.61     85443\n",
      "weighted avg       1.00      1.00      1.00     85443\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix = confusion_matrix(y_test, y_pred)\n",
    "print(confusion_matrix)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "af6ef14a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a function to report confusion metrics\n",
    "def confusion_metrics (conf_matrix):\n",
    "# save confusion matrix and slice into four pieces\n",
    "    TP = conf_matrix[1][1]\n",
    "    TN = conf_matrix[0][0]\n",
    "    FP = conf_matrix[0][1]\n",
    "    FN = conf_matrix[1][0]\n",
    "    print('True Positives:', TP)\n",
    "    print('True Negatives:', TN)\n",
    "    print('False Positives:', FP)\n",
    "    print('False Negatives:', FN)\n",
    "    \n",
    "    # calculate accuracy\n",
    "    conf_accuracy = (float (TP+TN) / float(TP + TN + FP + FN))\n",
    "    \n",
    "    # calculate the sensitivity\n",
    "    conf_sensitivity = (TP / float(TP + FN)) \n",
    "    # calculate the specificity\n",
    "    conf_specificity = (TN / float(TN + FP))\n",
    "    \n",
    "    # calculate precision\n",
    "    conf_precision = (TN / float(TN + FP))\n",
    "    # calculate f_1 score\n",
    "    conf_f1 = 2 * ((conf_precision * conf_sensitivity) / (conf_precision + conf_sensitivity))\n",
    "    print('-'*40)\n",
    "    print(f'Accuracy: {round(conf_accuracy * 100.0,4)}%') \n",
    "    print(f'Sensitivity: {round(conf_sensitivity * 100.0,4)}%') \n",
    "    print(f'Specificity: {round(conf_specificity * 100.0,4)}%') \n",
    "    print(f'Precision: {round(conf_precision * 100.0,4)}%')\n",
    "    print(f'f_1 Score: {round(conf_f1 * 100.0,4)}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "07139bf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Positives: 17\n",
      "True Negatives: 85305\n",
      "False Positives: 2\n",
      "False Negatives: 119\n",
      "----------------------------------------\n",
      "Accuracy: 99.8584%\n",
      "Sensitivity: 12.5%\n",
      "Specificity: 99.9977%\n",
      "Precision: 99.9977%\n",
      "f_1 Score: 22.2222%\n"
     ]
    }
   ],
   "source": [
    "confusion_metrics(confusion_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce23f485",
   "metadata": {},
   "source": [
    "Sensitivity (aka Recall) means out of all actual Positives, how many did we predict as Positive.\n",
    "\n",
    "Specificity (aka Selectivity or True Negative Rate, TNR) means out of all actual Negatives, how many did we predict as Negative.\n",
    "\n",
    "Precision (aka Positive Predictive Value, PPV) means â€œout of all predicted Positive cases, how many were actually Positive.\n",
    "\n",
    "F1 Score is the harmonic, or weighted average of Precision and Sensitivity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "455ab603",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.99831467 0.99884133 0.99961378 0.99852533 0.99831467 0.99856044\n",
      " 0.99957867 0.99852528 0.99863062 0.99870084]\n",
      "\n",
      "Average Cross Validation score= 0.9987605626794759\n"
     ]
    }
   ],
   "source": [
    "# cross validation score\n",
    "scores= cross_val_score(pipe, df1, Class, cv= 10)\n",
    "print(scores)\n",
    "print(f'\\nAverage Cross Validation score= {np.mean(scores)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51b6866f",
   "metadata": {},
   "source": [
    "Our data ran through the pipeline with cross validation appears to perform slightly better, however, the improved performance is insignificant. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
